# Multi-stage build for production
FROM node:18-alpine AS builder

# Install build dependencies
RUN apk add --no-cache python3 make g++

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install all dependencies (including dev dependencies for building)
RUN npm ci

# Copy source code
COPY . .

# Run any build steps if needed
# RUN npm run build

# Production stage
FROM node:18-alpine AS production

# Install Ollama in the production container
RUN apk add --no-cache curl bash
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Create non-root user for security
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nextjs -u 1001

WORKDIR /app

# Copy package files from builder stage
COPY --chown=nextjs:nodejs package*.json ./

# Install production dependencies only
RUN npm ci --omit=dev && npm cache clean --force

# Copy application code from builder stage
COPY --chown=nextjs:nodejs --from=builder /app ./

# Create necessary directories and set permissions
RUN mkdir -p /home/nextjs/.ollama /app/logs /app/data && \
    chown -R nextjs:nodejs /home/nextjs /app/logs /app/data

USER nextjs

# Expose ports
EXPOSE 3000
EXPOSE 11434

HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD wget --quiet --tries=1 --spider http://localhost:3000/health || exit 1

# Pull a default model during build (optional, can be skipped for smaller images)
RUN ollama serve & sleep 5 && timeout 10 ollama pull llama3 || echo "Model pull failed but continuing..."

# Start the application
CMD ["sh", "-c", "ollama serve & node index.js start"]